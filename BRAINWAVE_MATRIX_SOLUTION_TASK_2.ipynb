{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdonFBEqpc4C/IGTFxJ3dh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abir16-tech/BRAINWAVE_MATRIX_INTERN-/blob/main/BRAINWAVE_MATRIX_SOLUTION_TASK_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKpyQ3-11MV0",
        "outputId": "0a2260fe-c7b2-4038-fd36-65d15e40c26a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 80.9M  100 80.9M    0     0  52.6M      0  0:00:01  0:00:01 --:--:-- 67.2M\n",
            "   polarity                                               text\n",
            "0         0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
            "1         0  is upset that he can't update his Facebook by ...\n",
            "2         0  @Kenichan I dived many times for the ball. Man...\n",
            "3         0    my whole body feels itchy and like its on fire \n",
            "4         0  @nationwideclass no, it's not behaving at all....\n",
            "polarity\n",
            "0    800000\n",
            "1    800000\n",
            "Name: count, dtype: int64\n",
            "                                                     text  \\\n",
            "541200             @chrishasboobs AHHH I HOPE YOUR OK!!!    \n",
            "750     @misstoriblack cool , i have no tweet apps  fo...   \n",
            "766711  @TiannaChaos i know  just family drama. its la...   \n",
            "285055  School email won't open  and I have geography ...   \n",
            "705995                             upper airways problem    \n",
            "\n",
            "                                               clean_text  \n",
            "541200                               ahhh i hope your ok   \n",
            "750             cool  i have no tweet apps  for my razr 2  \n",
            "766711   i know  just family drama its lamehey next ti...  \n",
            "285055  school email wont open  and i have geography s...  \n",
            "705995                             upper airways problem   \n",
            "Train size: 1280000\n",
            "Test size: 320000\n",
            "TF-IDF shape (train): (1280000, 5000)\n",
            "TF-IDF shape (test): (320000, 5000)\n",
            "\n",
            "Bernoulli Naive Bayes Accuracy: 0.7658375\n",
            "\n",
            "BernoulliNB Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.74      0.76    160000\n",
            "           1       0.75      0.79      0.77    160000\n",
            "\n",
            "    accuracy                           0.77    320000\n",
            "   macro avg       0.77      0.77      0.77    320000\n",
            "weighted avg       0.77      0.77      0.77    320000\n",
            "\n",
            "\n",
            "SVM Accuracy: 0.7953375\n",
            "\n",
            "SVM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.78      0.79    160000\n",
            "           1       0.78      0.81      0.80    160000\n",
            "\n",
            "    accuracy                           0.80    320000\n",
            "   macro avg       0.80      0.80      0.80    320000\n",
            "weighted avg       0.80      0.80      0.80    320000\n",
            "\n",
            "\n",
            "Logistic Regression Accuracy: 0.795471875\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.78      0.79    160000\n",
            "           1       0.79      0.81      0.80    160000\n",
            "\n",
            "    accuracy                           0.80    320000\n",
            "   macro avg       0.80      0.80      0.80    320000\n",
            "weighted avg       0.80      0.80      0.80    320000\n",
            "\n",
            "\n",
            "Sample Predictions:\n",
            "BernoulliNB: [1 0 1]\n",
            "SVM: [1 0 1]\n",
            "Logistic Regression: [1 0 1]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "\n",
        "# Download the dataset zip file using the provided curl command\n",
        "# Note: Kaggle datasets often require authentication. This direct URL might not work without it.\n",
        "# A more robust approach for Kaggle data in Colab is using the Kaggle API.\n",
        "# However, I will use the provided curl command as requested.\n",
        "download_url = 'https://www.kaggle.com/api/v1/datasets/download/kazanova/sentiment140'\n",
        "zip_file_path = '/content/sentiment140.zip'\n",
        "extracted_csv_path = '/content/training.1600000.processed.noemoticon.csv'\n",
        "\n",
        "\n",
        "# Using shell command to download\n",
        "get_ipython().system(f'curl -L -o {zip_file_path} {download_url}')\n",
        "\n",
        "\n",
        "# Assuming the zip contains 'training.1600000.processed.noemoticon.csv'\n",
        "# We'll read directly from the zip\n",
        "file_path = zip_file_path # Pointing to the zip file\n",
        "\n",
        "# Check if the zip file was downloaded\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Error: Downloaded zip file not found at {file_path}. Please ensure the download was successful (Kaggle authentication might be required).\")\n",
        "else:\n",
        "    # Read directly from the zip file\n",
        "    # Using engine='python' and on_bad_lines='skip' for robustness\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, encoding='latin-1', header=None, compression='zip', on_bad_lines='skip', engine='python')\n",
        "        df = df[[0, 5]]\n",
        "        df.columns = ['polarity', 'text']\n",
        "        print(df.head())\n",
        "\n",
        "        # Filter out neutral sentiment (polarity 2)\n",
        "        df = df[df.polarity != 2]\n",
        "\n",
        "        # Map polarity values 0 to 0 and 4 to 1\n",
        "        df['polarity'] = df['polarity'].map({0: 0, 4: 1})\n",
        "\n",
        "        # Shuffle the DataFrame\n",
        "        df = df.sample(frac=1, random_state=42)\n",
        "\n",
        "        print(df['polarity'].value_counts())\n",
        "\n",
        "        def clean_text(text):\n",
        "            if text is None:  # Handle None values\n",
        "                return \"\"\n",
        "            text = text.lower()\n",
        "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
        "            text = re.sub(r'@\\w+', '', text) # Remove mentions\n",
        "            text = re.sub(r'#\\w+', '', text) # Remove hashtags\n",
        "            text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
        "            return text\n",
        "\n",
        "        df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "        print(df[['text', 'clean_text']].head())\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            df['clean_text'],\n",
        "            df['polarity'],\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=df['polarity'] # Added stratification to ensure both classes are in train/test splits\n",
        "        )\n",
        "\n",
        "        print(\"Train size:\", len(X_train))\n",
        "        print(\"Test size:\", len(X_test))\n",
        "\n",
        "        # Check if both classes are present in training data after split\n",
        "        if len(y_train.value_counts()) < 2:\n",
        "             print(\"Error: Training data does not contain both classes after split. Check data loading and filtering.\")\n",
        "        else:\n",
        "            vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "\n",
        "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "            X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "            print(\"TF-IDF shape (train):\", X_train_tfidf.shape)\n",
        "            print(\"TF-IDF shape (test):\", X_test_tfidf.shape)\n",
        "\n",
        "            # --- Model Training and Evaluation ---\n",
        "\n",
        "            # Bernoulli Naive Bayes\n",
        "            bnb = BernoulliNB()\n",
        "            bnb.fit(X_train_tfidf, y_train)\n",
        "            bnb_pred = bnb.predict(X_test_tfidf)\n",
        "            print(\"\\nBernoulli Naive Bayes Accuracy:\", accuracy_score(y_test, bnb_pred))\n",
        "            print(\"\\nBernoulliNB Classification Report:\\n\", classification_report(y_test, bnb_pred))\n",
        "\n",
        "            # Linear SVM\n",
        "            svm = LinearSVC(max_iter=1000)\n",
        "            svm.fit(X_train_tfidf, y_train)\n",
        "            svm_pred = svm.predict(X_test_tfidf)\n",
        "            print(\"\\nSVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
        "            print(\"\\nSVM Classification Report:\\n\", classification_report(y_test, svm_pred))\n",
        "\n",
        "            # Logistic Regression\n",
        "            logreg = LogisticRegression(max_iter=1000) # Increased max_iter\n",
        "            logreg.fit(X_train_tfidf, y_train)\n",
        "            logreg_pred = logreg.predict(X_test_tfidf)\n",
        "            print(\"\\nLogistic Regression Accuracy:\", accuracy_score(y_test, logreg_pred))\n",
        "            print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, logreg_pred))\n",
        "\n",
        "            # --- Sample Predictions ---\n",
        "            sample_tweets = [\"I love this!\", \"I hate that!\", \"It was okay, not great.\"]\n",
        "            # Clean sample tweets before vectorizing and predicting\n",
        "            sample_vec = vectorizer.transform([clean_text(tweet) for tweet in sample_tweets])\n",
        "\n",
        "            print(\"\\nSample Predictions:\")\n",
        "            print(\"BernoulliNB:\", bnb.predict(sample_vec))\n",
        "            print(\"SVM:\", svm.predict(sample_vec))\n",
        "            print(\"Logistic Regression:\", logreg.predict(sample_vec))\n",
        "\n",
        "    except FileNotFoundError:\n",
        "         print(f\"Error: Expected CSV file not found within the zip at {extracted_csv_path}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during file processing or model training: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "6e5Iun-8Blvy"
      }
    }
  ]
}